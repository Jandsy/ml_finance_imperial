{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jandsy/ml_finance_imperial/blob/main/Programming_Sessions/Programming_session_6/Programming_Session_GLOVE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A84sSMXE3hbZ"
      },
      "source": [
        "\n",
        "# **<center>Machine Learning and Finance </center>**\n",
        "\n",
        "\n",
        "## <center> Programming Sessing 5: GloVe Implementation - </center>\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# Glove Implementation:\n"
      ],
      "metadata": {
        "id": "i3UNZ1BIqRBM"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jNbuq3vZIg-Q"
      },
      "source": [
        "\n",
        "\n",
        "## Introduction\n",
        "In this programming session, we aim to implement the GloVe approach, as proposed by Jeffrey Pennington, Richard Socher, and Christopher D. Manning in their paper titled [GloVe: Global Vectors for Word Representation](https://nlp.stanford.edu/pubs/glove.pdf).\n",
        "\n",
        "## Session Overview\n",
        "The programming session is divided into three main sections, each with specific objectives:\n",
        "\n",
        "### Section 1: Data Loading and Preprocessing\n",
        "- **Objective**: Load the data, preprocess it, and create the co-occurrence matrix.\n",
        "- **Tasks**:\n",
        "    - Load the dataset.\n",
        "    - Preprocess the text data (e.g., tokenization, lowercasing, removing stopwords, etc.).\n",
        "    - Construct the co-occurrence matrix based on word occurrences in the corpus.\n",
        "\n",
        "### Section 2: Model Training\n",
        "- **Objective**: Train the GloVe model using two different methods: Gradient Descent and Alternating Least Squares (ALS).\n",
        "- **Tasks**:\n",
        "    - Implement the GloVe model.\n",
        "    - Train the model using Gradient Descent.\n",
        "    - Train the model using Alternating Least Squares.\n",
        "    \n",
        "### Section 3: Regularization Techniques\n",
        "- **Objective**: Enhance the GloVe model by incorporating a penalty term to the loss function as a regularization technique.\n",
        "- **Tasks**:\n",
        "    - Define the loss function with a penalty term.\n",
        "    - Integrate the regularization technique into the model training process.\n",
        "    \n",
        "\n",
        "**Notations:**\n",
        "\n",
        "* $\\mathcal{M}_{n,p}(\\mathbb{R})$ is the space of the matrices composed of n rows and p columns.\n",
        "\n",
        "* $I_n \\in \\mathcal{M}_{n,n}(\\mathbb{R})$ is the identity matrix of size n.\n",
        "\n",
        "* For all $z \\in \\mathbb{R}^D$, the $\\mathcal{L}^2$ norm on $\\mathbb{R}^D$ of $z$ is defined as follows: $||z||_2^2 = z^T z$\n",
        "\n",
        "*  For all $A = [a_{ij}]_{i,j} \\in \\mathcal{M}_{n,p}(\\mathbb{R})$ we define the Frobenius norm of $A$ as follows: $||A||_{\\text{F}}^2 = \\sum\\limits_{i=1}^n \\sum\\limits_{j=1}^p a_{ij}^2$\n",
        "\n",
        "* The gradient of a function $f : \\theta \\in \\mathbb{R}^D \\mapsto \\mathbb{R}$ at $\\theta\\in \\mathbb{R}^D$ is denoted as follows $\\nabla_{\\theta}f(\\theta) = \\left(\\frac{\\partial f}{\\partial \\theta_1}(\\theta), \\dots, \\frac{\\partial f}{\\partial \\theta_D}(\\theta) \\right)$\n",
        "\n",
        "**Convention:**\n",
        "\n",
        "* The rows $(A_i)_{1 \\leq i \\leq n }$ of a matrix $A = \\begin{pmatrix}\n",
        "- & A_1 & - \\\\\n",
        "\\vdots & \\vdots & \\vdots \\\\\n",
        "- & A_n & -\n",
        "\\end{pmatrix}\\in \\mathcal{M}_{n,p}(\\mathbb{R}) $ are \t\t\t\tconsidered $\\mathcal{M}_{p,1}(\\mathbb{R})$ matrices.\n",
        "\n",
        "* The columns $(B_j)_{1 \\leq j \\leq p }$ of a matrix $B = \\begin{pmatrix}\n",
        "| & \\dots & | \\\\\n",
        "B_1 & \\dots & B_p \\\\\n",
        "| & \\dots & |\n",
        "\\end{pmatrix}\\in \\mathcal{M}_{n,p}(\\mathbb{R}) $ are considered $\\mathcal{M}_{n,1}(\\mathbb{R})$ matrices.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kCcWOvm03Zxu"
      },
      "source": [
        "# Import basic libraries\n",
        "import pandas as pd # for dataframes\n",
        "import numpy as np # for arrays\n",
        "import matplotlib.pyplot as plt # for plots\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer # for processing text\n",
        "import random # to shuffle the sequences\n",
        "import os\n",
        "plt.style.use('dark_background') # to adapt the colors to a dark background\n",
        "from IPython.display import Image # for showing graphs from the lectures"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8yDVjd7FbF9L"
      },
      "source": [
        "# Hyperparameters\n",
        "MAX_VOCAB = 999\n",
        "C = 10 # Context size\n",
        "V = MAX_VOCAB + 1 # Vocabulary size\n",
        "EPOCHS = 64\n",
        "D = 100 # Embedding dimension"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZEg4IWrPY6QE"
      },
      "source": [
        "# 1. Getting the statistics of the word occurences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_myLbIjnJQx4"
      },
      "source": [
        "## 1.1 Introducing the problem"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7pf_Enu1KgPy"
      },
      "source": [
        "The main goal of this programming session is to develop a model trained on a corpus of sentences to represent words in a $D$-dimensional space. Our aim is to capture the semantic similarity between words directly within the embedding vectors themselves.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-SutTRTQJT4J"
      },
      "source": [
        "\n",
        "---\n",
        "<font color=green>Q1:</font>\n",
        "<br><font color='green'>\n",
        "Clarify why the concept of similarity is not captured in the one-hot vector representation of words.\n",
        "</font>\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VDVPk01tOsfT"
      },
      "source": [
        "---\n",
        "**Solution**:\n",
        "\n",
        "Like explained in Slide 7 of [Lecture 5], any two V-dimensional one hot vectors will be orthogonal according to the dot product similarty measure.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VmoXLYiyKlfr"
      },
      "source": [
        "Several methods have been used to create word embeddings. The most popular ones rely on the intuition that *a word's meaning is given by the words that frequently appear close-by*.\n",
        "\n",
        "For instance, [the word2vec approach](https://arxiv.org/pdf/1301.3781.pdf) represents the tokens as parameters of a shallow neural network predicting a word's context given the world itself.\n",
        "\n",
        "Although the shallow window-based model captures linguistic patterns between word vectors and performs well on the word analogy task ($w_{\\text{France}} - w_{\\text{Paris}} \\approx w_{\\text{England}} - w_{\\text{London}}$), the model suffers from the disadvantage that they do not operate directly and the co-occurence statistics.\n",
        "\n",
        "As explained in [Lecture 5], the GloVe method is a popular method used to learn low-dimensional word representations by using **matrix factorization** methods on a matrix of word-word **co-occurence** statistics."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qn9LBGBIj9K6"
      },
      "source": [
        "## 1.2 Preprocessing the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bG-5uQsmTf9L"
      },
      "source": [
        "The **data** folder contains a csv file named `RedditNews.csv` (Source: Sun, J. (2016, August) Daily News for Stock Market Prediction, Version 1. Retrieved [26 may 2020]).\n",
        "\n",
        "In the `RedditNews.csv` file are stored historical news headlines from Reddit WorldNews Channel, ranked by reddit users' votes, and only the top 25 headlines are considered for a single date.\n",
        "\n",
        "You will find two colomns:\n",
        "\n",
        "\n",
        "* The first column is for the \"date\".\n",
        "* The second column is for the \"News\". As all the news are ranked from top to bottom, there are only 25 lines for each date.  "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "<font color=green>Q2:</font>\n",
        "<br><font color='green'>\n",
        "Load the data stored in the CSV file and generate a list containing all the news 'all_news'.\n",
        "</font>\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "k6wCO8hAF3Vo"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m-JxcrF8Khhs"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IhGBiEn-BOTy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CSiWG1LOTH8-"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "<font color=green>Q3:</font>\n",
        "<br><font color='green'>\n",
        "You are given a list of news articles stored in a variable called news. Your task is to preprocess this text data using `Tokenizer` module. Specifically, you need to:\n",
        "\n",
        "1.Initialize a Tokenizer object with the following parameters:\n",
        "\n",
        "* num_words: Set to the variable MAX_VOCAB which specifies the maximum number of words to keep, based on word frequency.\n",
        "\n",
        "* oov_token: Set to 'UNK' which stands for \"out-of-vocabulary\" token. This token will replace any word not in the MAX_VOCAB most frequent words.\n",
        "\n",
        "* filters: A string of characters to filter out from the texts, set to '!\"#$%&()*+,-./:;<=>?@[\\\\]^_{|}~\\t\\n'`.\n",
        "\n",
        "* lower: Convert texts to lowercase, set to True.\n",
        "\n",
        "2.Fit the Tokenizer on the news data to create the word index dictionary using the fit_on_texts method.\n",
        "\n",
        "3.Transform the news articles into a list of lists called `new_processed` of integers using the texts_to_sequences method, where each integer represents the index of a word in the Tokenizer word index.\n",
        "</font>\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "covqA1a_xVqe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "<center><img width=\"400\" src = \"https://drive.google.com/uc?export=view&id=15_ocFu9iG0sOPmK0dKhECzTUbIXFSeHC\"></center>"
      ],
      "metadata": {
        "id": "s5grXRA_CgxV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "<font color=green>Q4:</font>\n",
        "<br><font color='green'>\n",
        "You have preprocessed a list of news articles and converted them into sequences of integers stored in `news_processed`. Your tasks are as follows:\n",
        "\n",
        "1. **Create a custom word index dictionary**:\n",
        "   - Initialize an empty dictionary called `word_index`.\n",
        "   - Add a special token `'<sos>'` (start of sentence) with an index of `0`.\n",
        "   - Iterate over the items in the `tokenizer.word_index` dictionary and add each word and its index to `word_index` only if its index is less than `MAX_VOCAB`.\n",
        "   - Add another special token `'EOS'` (end of sentence) with an index of `MAX_VOCAB`.\n",
        "\n",
        "2. **Shuffle the processed news sequences**:\n",
        "   - Use the `random.shuffle` method to randomly shuffle the list `news_processed`.\n",
        "\n",
        "3. **Add special tokens to each sequence**:\n",
        "   - For each sequence in `news_processed`, prepend the index of `'<sos>'` (which is `0`) to the beginning of the sequence and append the index of `'EOS'` (which is `MAX_VOCAB`) to the end of the sequence.\n",
        "\n",
        "</font>\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "gsuspkmgDvd3"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "trNDY43QyjlC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BOKn97_H6Qd6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cyal4krnNQc-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37bxb3akkE4B"
      },
      "source": [
        "## 1.3 Creating the co-occurence matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bMXi6HniTk6j"
      },
      "source": [
        "Let $V$ be the vocabulary size of the training corpus.\n",
        "\n",
        "In [Lecture 5](https://mlfbg.github.io/MachineLearningInFinance/Lectures/Lecture_5.pdf), we have defined the co-occurence matrix $X = [X_{ij}]_{i,j} \\in \\mathcal{M}_{V,V}(\\mathbb{R})$, whose entries $X_{ij}$ represent the number of times word $j$ appears in the context of word $i$.\n",
        "\n",
        "Algorithm 1 summarizes the steps involved in estimating the co-occurence matrix from the corpus `sequences`.\n",
        "\n",
        "<center><img width=\"400\" src = \"https://drive.google.com/uc?export=view&id=186c4b_X8mDEgBoVNZKEw7sfXOzKa-KN-\"></center>\n",
        "\n",
        "\n",
        "In Algorithm 1, each time a word $w[j]$ (of index $j$ in sequence) appears in the context of a center word $w[i]$ (of index $i$ in sequence), we increase the value of $X[w[i], w[j]]$ by a value of $1$ regardless of how close the word $w[j]$ is to the word $w[i]$.\n",
        "\n",
        "We would like to take into consideration the distance $d(i,j)$ between the center word $w[i]$ and the context word $w[j]$ when updating the value $X[w[i], w[j]]$, as shown the following figure:\n",
        "\n",
        "<center><img width=\"600\" src = \"https://drive.google.com/uc?export=view&id=1m1_32ovMfjkRVb-B_3gPizDI1QUfZjQ4\"></center>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "<font color=green>Q5:</font>\n",
        "<br><font color='green'>\n",
        "Explain why it makes more sense to use the following update equation for $X[w[i], w[j]]$ when word $w[j]$ of index $j$ is in the context word $w[i]$ of index $i$.\n",
        "\n",
        "\\begin{equation*}\n",
        "X[w[i], w[j]] \\longleftarrow X[w[i], w[j]] + \\frac{1}{|i-j|}\n",
        "\\end{equation*}\n",
        "</font>\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "e1M45t0GD0xC"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OVnkACgmLcub"
      },
      "source": [
        "---\n",
        "**Solution**\n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "<font color=green>Q6:</font>\n",
        "<br><font color='green'>\n",
        "Implement Algorithm 2 to get the co-occurence matrix $X$ using a function called `get_cooccurence_matrix()`.\n",
        "\n",
        "The function  should accept `sequences`, `context_size` and `vocabulary_size` as input arguments and output the matrix `X`.\n",
        "</font>\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "<center><img width=\"400\" src = \"https://drive.google.com/uc?export=view&id=1Xt6SXVNxlsPHqIWk1IIaLOZcpTrQWZSd\"></center>\n",
        "\n"
      ],
      "metadata": {
        "id": "XI-bKFPhD2Bq"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rZ11kDuA4yav"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-zM372IuTZlr"
      },
      "source": [
        "\n",
        "\n",
        "Given that the non-zero values in matrix $X$ are substantial, we apply the logarithmic function to all elements of $X$, after adding 1 to each entry $X_{ij}$ to prevent taking the logarithm of zero values. The resultant matrix remains sparse and is denoted as $\\log X$.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "<font color=green>Q7:</font>\n",
        "<br><font color='green'>\n",
        "Create the matrix $\\log X$, call it `logX`.\n",
        "</font>\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "U4c7UN5CD3ba"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s8ANrmOAOEb8"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RNTHnF-LkFFK"
      },
      "source": [
        "# 2. Training the weighted least squares regression model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GHIrgh9bkFM-"
      },
      "source": [
        "## 2.1 Introducing the cost function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r59PLdpZkFXS"
      },
      "source": [
        "The logarithm of the co-occurence matrix $\\log X$ has been defined in the previous section. The objective of this section is to approximate $\\log X$ using a factorization method as follows:\n",
        "\n",
        "\\begin{equation*}\n",
        "\t\\forall (i,j) \\in \\{1, \\dots, V \\}^2 \\quad \\log X_{ij} \\approx W_i^T \\tilde{W}_j + b_i + \\tilde{b}_j\n",
        "\\end{equation*}\n",
        "\n",
        "\n",
        "The parameters of the regression model are:\n",
        "\n",
        "\n",
        "* A first **embedding matrix** and a bias term associated with it:\n",
        "\t\\begin{equation*}\n",
        "W = \\begin{pmatrix}\n",
        "- & W_1 & - \\\\\n",
        "\\vdots & \\vdots & \\vdots \\\\\n",
        "- & W_V & -\n",
        "\\end{pmatrix}\\in \\mathcal{M}_{V, D}(\\mathbb{R}), \t\\quad  \tb = \\begin{pmatrix}\n",
        " b_1  \\\\\n",
        " \\vdots  \\\\\n",
        " b_V  \n",
        "\\end{pmatrix}\\in \\mathbb{R}^{V}\n",
        "\t\\end{equation*}\n",
        "\n",
        "\n",
        "* A second **embedding matrix** and a bias term associated with it:\n",
        "\t\\begin{equation*}\n",
        "\\tilde{W} = \\begin{pmatrix}\n",
        "- & \\tilde{W}_1 & - \\\\\n",
        "\\vdots & \\vdots & \\vdots \\\\\n",
        "- & \\tilde{W}_V & -\n",
        "\\end{pmatrix}\\in \\mathcal{M}_{V, D}(\\mathbb{R}), \\quad \\tilde{b} = \\begin{pmatrix}\n",
        " \\tilde{b}_1  \\\\\n",
        " \\vdots  \\\\\n",
        " \\tilde{b}_V  \n",
        "\\end{pmatrix}\\in \\mathbb{R}^{V}\n",
        "\t\\end{equation*}\n",
        "\n",
        "\n",
        "Instead of equal-weighting all the co-occurences, we introduce a **weighting function** $f(X_{ij})$ defined as follows:\n",
        "\n",
        "\\begin{equation*}\n",
        "\\forall x \\in \\mathbb{R}_{+} \\quad f(x) =  \\begin{cases}\n",
        "      (x/x_{\\text{max}})^{\\alpha} & \\text{if   $x < x_{\\text{max}}$}\\\\\n",
        "      1 & \\text{otherwise}\n",
        "          \\end{cases}  \n",
        "\\end{equation*}\n",
        "\n",
        "\n",
        "The function $f$ is represented in the following figure with $x_{\\text{max}}=100$ and $\\alpha=0.75$\n",
        "\n",
        "<center><img width=\"400\" src = \"https://drive.google.com/uc?export=view&id=1D7muXkREj-5pPVUWyAfe8qrwYeIe0XzN\"></center>\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "<font color=green>Q8:</font>\n",
        "<br><font color='green'>\n",
        "Create a matrix of shape $(V, V)$ whose entries are $f(X_{ij})$.\n",
        "Let's call it `fX`. Use the hyperparameters $x_{\\text{max}}=100$ and $\\alpha=0.75$\n",
        "</font>\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "dBlWZ8ZsD4qx"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YMvkPXeqrjcA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yEnJbfO6ObLg"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "<font color=green>Q9:</font>\n",
        "<br><font color='green'>\n",
        "What are the hyperparameters associated with the weighting function and what is the underlying reasoning behind its incorporation into the model?\n",
        "</font>\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "1ofPy91eD5UN"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FWbvjvDsO8Cy"
      },
      "source": [
        "---\n",
        "**Solution:**\n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vjWn9PfcTzqW"
      },
      "source": [
        "The **cost function** can then be written as follows:\n",
        "\n",
        "\\begin{equation*}\n",
        "J = \\sum_{i=1}^V \\sum_{j=1}^V f(X_{ij}) (\\log X_{ij} - W_i^T \\tilde{W}_j - b_i - \\tilde{b}_j)^2\n",
        "\\end{equation*}\n",
        "\n",
        "\n",
        "The gradients of the cost function $J$ with respect to all the parameters are introduced in the following equations:\n",
        "\n",
        "\n",
        "For all $i \\in \\{1, \\dots, V \\}$ and all $j \\in \\{ 1, \\dots, V \\}$:\n",
        "\n",
        "\n",
        "\\begin{align}\n",
        "& \\nabla_{W_i} J(W_i) = -2 \\sum_{j'=1}^V f(X_{ij'}) \\left( \\log X_{ij'} - W_i^T \\tilde{W}_{j'} - b_i - \\tilde{b}_{j'} \\right) \\tilde{W}_{j'} \\quad \\text{(2.1)} \\\\\n",
        "& \\nabla_{\\tilde{W}_j} J(W_j) = -2 \\sum_{i'=1}^V f(X_{i' j}) \\left( \\log X_{i' j} - W_{i'}^T \\tilde{W}_j - b_{i'} - \\tilde{b}_j \\right) W_{i'} \\quad \\text{(2.2)}  \\\\\n",
        "&\\nabla_{b_i} J(b_i) = -2 \\sum_{j'=1}^V f(X_{ij'}) \\left( \\log X_{ij'} - W_i^T \\tilde{W}_{j'} - b_i - \\tilde{b}_{j'} \\right) \\quad \\text{(2.3)}  \\\\\n",
        "& \\nabla_{\\tilde{b}_j} J(\\tilde{b}_j) = -2 \\sum_{i'=1}^V f(X_{i' j}) \\left( \\log X_{i' j} - W_{i'}^T \\tilde{W}_j - b_{i'} - \\tilde{b}_j \\right) \\quad \\text{(2.4)}\n",
        "\\end{align}\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "<font color=green>Q10:</font>\n",
        "<br><font color='green'>\n",
        "What is the total number of parameters in the model ? What are the shapes of all the gradients introduced in the equations (2.1), (2.2), (2.3) and (2.4) ?\n",
        "</font>\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "XbzQWdyLD6Q2"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MDaCk9RMdYAr"
      },
      "source": [
        "---\n",
        "**Solution:**\n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0rZEuCrPT3cH"
      },
      "source": [
        "We will introduce two training methods:\n",
        "\n",
        "* The first training method, called **alternating least squares**, involves deriving the update equations by equating all gradients to zero.\n",
        "* The second training method consists in applying the **gradient descent** algorithm.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nbPZ_vBQkFdg"
      },
      "source": [
        "## 2.2 Alternating least squares"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-i1DbdJkFib"
      },
      "source": [
        "We would like to estimate the parameters $W, \\tilde{W}, b, \\tilde{b}$ by setting the gradients to zero.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "By setting the gradients to zero, we get the following update equations:\n",
        "\n",
        "\\begin{align*}\n",
        "&\\nabla_{W_i} J(W_i) = 0 \\iff W_i = \\left( \\sum_{j'=1}^V f(X_{ij'}) \\tilde{W}_{j'} \\tilde{W}_{j'}^T \\right)^{-1} \\left( \\sum_{j'=1}^{V} f(X_{ij'})( \\log X_{ij'} - b_i - \\tilde{b}_{j'}) \\tilde{W}_{j'} \\right)  \\\\\n",
        "&\\nabla_{\\tilde{W}_j} J(\\tilde{W}_j) = 0 \\iff \\tilde{W}_j = \\left( \\sum_{i'=1}^V f(X_{i' j}) W_{i'} W_{i'}^T \\right)^{-1} \\left( \\sum_{i'=1}^{V} f(X_{i' j})( \\log X_{i' j} - b_{i'} - \\tilde{b}_{j}) W_{i'} \\right)  \\\\\n",
        "&\\nabla_{b_i} J(b_i) = 0 \\iff b_i = \\left( \\sum_{j'=1}^V f(X_{ij'})  \\right)^{-1} \\left( \\sum_{j'=1}^{V} f(X_{ij'})( \\log X_{ij'} - W_i^T \\tilde{W}_{j'} - \\tilde{b}_{j'}) \\right)  \\\\\n",
        "&\\nabla_{\\tilde{b}_j} J(\\tilde{b}_j) = 0 \\iff \\tilde{b}_j = \\left( \\sum_{i'=1}^V f(X_{i' j})  \\right)^{-1} \\left( \\sum_{i'=1}^{V} f(X_{i' j})( \\log X_{i' j} - W_{i'}^T \\tilde{W}_{j} - b_{i'}) \\right)\n",
        "\\end{align*}\n",
        "\n",
        "\n",
        "The proof can be found in **appendix A**"
      ],
      "metadata": {
        "id": "InI0dBGgD7tN"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FO0xwC4eXwAp"
      },
      "source": [
        "Each update equation for one parameter is a function of the other parameters. Therefore, in order to train our model, we can choose a number of iterations $N_{\\text{epochs}}$, and apply the update equations $N_{\\text{epochs}}$ times by keeping track of the loss to make sure it converges.\n",
        "\n",
        "For each iteration step $t \\in \\{0, \\dots, N_{\\text{epochs}}-1 \\}$, let $W^{(t)}, \\tilde{W}^{(t)}, b^{(t)}, \\tilde{b}^{(t)}$ represent the parameters of our model at the iteration $t$.\n",
        "\n",
        "\n",
        "The update equations from iteration $t$ to $t+1$ can then be written as follows:\n",
        "\n",
        "\\begin{align}\n",
        "&W_i^{(t+1)} \\longleftarrow \\left( \\sum_{j'=1}^V f(X_{ij'}) \\tilde{W}_{j'}^{(t)} \\tilde{W}_{j'}^{(t)^T} \\right)^{-1} \\left( \\sum_{j'=1}^{V} f(X_{ij'})( \\log X_{ij'} - b_i^{(t)} - \\tilde{b}_{j'}^{(t)}) \\tilde{W}_{j'}^{(t)} \\right) \\quad \\text{(2.5)} \\\\\n",
        "&\\tilde{W}_j^{(t+1)} \\longleftarrow \\left( \\sum_{i'=1}^V f(X_{i' j}) W_{i'}^{(t)} W_{i'}^{(t)^T} \\right)^{-1} \\left( \\sum_{i'=1}^{V} f(X_{i' j})( \\log X_{i' j} - b_{i'}^{(t)} - \\tilde{b}_{j}^{(t)}) W_{i'}^{(t)} \\right) \\quad \\text{(2.6)}  \\\\\n",
        "&b_i^{(t+1)} \\longleftarrow \\left( \\sum_{j'=1}^V f(X_{ij'})  \\right)^{-1} \\left( \\sum_{j'=1}^{V} f(X_{ij'})( \\log X_{ij'} - W_i^{(t)^T} \\tilde{W}_{j'}^{(t)} - \\tilde{b}_{j'}^{(t)}) \\right) \\quad \\text{(2.7)} \\\\\n",
        "&\\tilde{b}_j^{(t+1)} \\longleftarrow \\left( \\sum_{i'=1}^V f(X_{i' j})  \\right)^{-1} \\left( \\sum_{i'=1}^{V} f(X_{i' j})( \\log X_{i' j} - W_{i'}^{(t)^T} \\tilde{W}_{j}^{(t)} - b_{i'}^{(t)}) \\right) \\quad \\text{(2.8)}\n",
        "\\end{align}\n",
        "\n",
        "\n",
        "\n",
        "The pseudo code for the training algorithm can be expressed as follows:\n",
        "\n",
        "\n",
        "<center><img width=\"500\" src = \"https://drive.google.com/uc?export=view&id=1DQmP3N13RH2hAP2-Szgk8TPhzcHICwGU\"></center>\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "<font color=green>Q11:</font>\n",
        "<br><font color='green'>\n",
        "Implement the alternating least squares training algorithm\n",
        "</font>\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "my9j51piD8-d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Optimization Hyperparameters\n",
        "learning_rate=1e-4\n",
        "epochs = EPOCHS\n",
        "\n",
        "# initialize weights\n",
        "W = np.random.randn(V, D) / np.sqrt(V + D)\n",
        "b = np.zeros(V)\n",
        "W_tilde = np.random.randn(V, D) / np.sqrt(V + D)\n",
        "b_tilde = np.zeros(V)\n",
        "\n",
        "\n",
        "\n",
        "costs = []\n",
        "for epoch in range(epochs):\n",
        "    # epsilon (V, V) matrix such that epsilon_{ij} = logX_{ij} - W_i^T W_tilde_j - b_i - b_tilde_j\n",
        "    epsilon = logX - W.dot(W_tilde.T) - b.reshape(V, 1) - b_tilde.reshape(1, V)\n",
        "    # cost function sum_{ij} fX_{ij} (logX_{ij} - W_i^T W_tilde_j - b_i - b_tilde_j)^2 = sum_{ij} fX_{ij} epsilon_{ij}^2\n",
        "    cost = (fX * epsilon * epsilon).sum()\n",
        "    costs.append(cost)\n",
        "    print(\"epoch: {}...cost: {}\".format(epoch, cost))\n",
        "\n",
        "    # update W\n",
        "    print(\"Update W..\")\n",
        "    for i in range(V):\n",
        "        if i%1000==0:\n",
        "            print(\"Epoch {}... W is updated for {} words out of {}\".format(epoch, i, V))\n",
        "        # A = sum_{j'} fX_{ij'} W_tilde_j'  W_tilde_j'^T\n",
        "        A = (fX[i,:].reshape(1,V)*W_tilde.T).dot(W_tilde) # or A = (W_tilde.T).dot(W_tilde*(fX[i, :][:, None]))\n",
        "        # B = sum_{j'} fX_{ij'} (logX_{ij'} - b_i - b_tilde_j') W_tilde_j'\n",
        "        B = (fX[i, :]*(logX[i, :] - b[i] - b_tilde)).dot(W_tilde)\n",
        "        # W_i = A^{-1} B\n",
        "        W[i] = np.linalg.solve(A, B)\n",
        "\n",
        "\n",
        "    # update b\n",
        "    print(\"Update b..\")\n",
        "    for i in range(V):\n",
        "        if i%1000==0:\n",
        "            print(\"Epoch {}... b is updated for {} words out of {}\".format(epoch, i, V))\n",
        "        # A = sum_{j'} fX_{ij'}\n",
        "        A = fX[i, :].sum()\n",
        "        # B = sum_{j'} fX_{ij'} (logX_{ij'} - W_i^T W_tilde_j' - b_tilde_j')\n",
        "        B = fX[i, :].dot(logX[i, :] - W[i].dot(W_tilde.T) - b_tilde)\n",
        "        # b_i = A^{-1} B\n",
        "        b[i] = B/A\n",
        "\n",
        "    # update W_tilde\n",
        "    print(\"Update W_tilde..\")\n",
        "    for j in range(V):\n",
        "        if j%1000==0:\n",
        "            print(\"Epoch {}... W_tilde is updated for {} words out of {}\".format(epoch, j, V))\n",
        "        # A = sum_{i'} fX_{i'j} W_i'  W_i'^T\n",
        "        A = (fX[:, j][None, :]*(W.T)).dot(W)\n",
        "        # B = sum_{i'} fX_{i'j} (logX_{i'j} - b_i' - b_tilde_j) W_i'\n",
        "        B = (fX[:, j]*(logX[:, j] - b - b_tilde[j])).dot(W)\n",
        "        # W_tilde_j = A^{-1} B\n",
        "        W_tilde[j] = np.linalg.solve(A, B)\n",
        "\n",
        "\n",
        "    # update b_tilde\n",
        "    print(\"Update b_tilde..\")\n",
        "    for j in range(V):\n",
        "        if j%1000==0:\n",
        "            print(\"Epoch {}... b_tilde is updated for {} words out of {}\".format(epoch, j, V))\n",
        "        # A = sum_{i'} fX_{i'j}\n",
        "        A = fX[:, j].sum()\n",
        "        # B = sum_{i'} fX_{i'j} (logX_{i'j} - W_i'^T W_tilde_j - b_i')\n",
        "        B = fX[:, j].dot(logX[:, j] - W.dot(W_tilde[j]) - b)\n",
        "        # b_tilde_j = A^{-1} B\n",
        "        b_tilde[j] = B/A\n",
        "\n"
      ],
      "metadata": {
        "id": "62nQJ9gmaNk_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72fafc16-e55a-4d66-fddf-26dbbe17c6cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 0...cost: 436187.3622280021\n",
            "Update W..\n",
            "Epoch 0... W is updated for 0 words out of 1000\n",
            "Update b..\n",
            "Epoch 0... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 0... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 0... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 1...cost: 5178.433494547229\n",
            "Update W..\n",
            "Epoch 1... W is updated for 0 words out of 1000\n",
            "Update b..\n",
            "Epoch 1... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 1... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 1... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 2...cost: 2810.3981103135634\n",
            "Update W..\n",
            "Epoch 2... W is updated for 0 words out of 1000\n",
            "Update b..\n",
            "Epoch 2... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 2... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 2... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 3...cost: 2482.528124412446\n",
            "Update W..\n",
            "Epoch 3... W is updated for 0 words out of 1000\n",
            "Update b..\n",
            "Epoch 3... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 3... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 3... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 4...cost: 2365.2409509424183\n",
            "Update W..\n",
            "Epoch 4... W is updated for 0 words out of 1000\n",
            "Update b..\n",
            "Epoch 4... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 4... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 4... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 5...cost: 2307.608435376434\n",
            "Update W..\n",
            "Epoch 5... W is updated for 0 words out of 1000\n",
            "Update b..\n",
            "Epoch 5... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 5... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 5... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 6...cost: 2273.754497610356\n",
            "Update W..\n",
            "Epoch 6... W is updated for 0 words out of 1000\n",
            "Update b..\n",
            "Epoch 6... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 6... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 6... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 7...cost: 2251.5073422450164\n",
            "Update W..\n",
            "Epoch 7... W is updated for 0 words out of 1000\n",
            "Update b..\n",
            "Epoch 7... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 7... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 7... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 8...cost: 2235.731570132557\n",
            "Update W..\n",
            "Epoch 8... W is updated for 0 words out of 1000\n",
            "Update b..\n",
            "Epoch 8... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 8... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 8... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 9...cost: 2223.9177556301584\n",
            "Update W..\n",
            "Epoch 9... W is updated for 0 words out of 1000\n",
            "Update b..\n",
            "Epoch 9... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 9... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 9... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 10...cost: 2214.7267415621986\n",
            "Update W..\n",
            "Epoch 10... W is updated for 0 words out of 1000\n",
            "Update b..\n",
            "Epoch 10... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 10... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 10... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 11...cost: 2207.388074359565\n",
            "Update W..\n",
            "Epoch 11... W is updated for 0 words out of 1000\n",
            "Update b..\n",
            "Epoch 11... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 11... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 11... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 12...cost: 2201.4213132252853\n",
            "Update W..\n",
            "Epoch 12... W is updated for 0 words out of 1000\n",
            "Update b..\n",
            "Epoch 12... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 12... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 12... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 13...cost: 2196.5017870388147\n",
            "Update W..\n",
            "Epoch 13... W is updated for 0 words out of 1000\n",
            "Update b..\n",
            "Epoch 13... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 13... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 13... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 14...cost: 2192.395529828336\n",
            "Update W..\n",
            "Epoch 14... W is updated for 0 words out of 1000\n",
            "Update b..\n",
            "Epoch 14... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 14... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 14... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 15...cost: 2188.927566272719\n",
            "Update W..\n",
            "Epoch 15... W is updated for 0 words out of 1000\n",
            "Update b..\n",
            "Epoch 15... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 15... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 15... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 16...cost: 2185.9652701705236\n",
            "Update W..\n",
            "Epoch 16... W is updated for 0 words out of 1000\n",
            "Update b..\n",
            "Epoch 16... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 16... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 16... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 17...cost: 2183.4080293708143\n",
            "Update W..\n",
            "Epoch 17... W is updated for 0 words out of 1000\n",
            "Update b..\n",
            "Epoch 17... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 17... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 17... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 18...cost: 2181.1795127859286\n",
            "Update W..\n",
            "Epoch 18... W is updated for 0 words out of 1000\n",
            "Update b..\n",
            "Epoch 18... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 18... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 18... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 19...cost: 2179.2212972947777\n",
            "Update W..\n",
            "Epoch 19... W is updated for 0 words out of 1000\n",
            "Update b..\n",
            "Epoch 19... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 19... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 19... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 20...cost: 2177.487756000822\n",
            "Update W..\n",
            "Epoch 20... W is updated for 0 words out of 1000\n",
            "Update b..\n",
            "Epoch 20... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 20... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 20... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 21...cost: 2175.942362318067\n",
            "Update W..\n",
            "Epoch 21... W is updated for 0 words out of 1000\n",
            "Update b..\n",
            "Epoch 21... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 21... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 21... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 22...cost: 2174.5552887878084\n",
            "Update W..\n",
            "Epoch 22... W is updated for 0 words out of 1000\n",
            "Update b..\n",
            "Epoch 22... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 22... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 22... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 23...cost: 2173.3019163822064\n",
            "Update W..\n",
            "Epoch 23... W is updated for 0 words out of 1000\n",
            "Update b..\n",
            "Epoch 23... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 23... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 23... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 24...cost: 2172.161862781303\n",
            "Update W..\n",
            "Epoch 24... W is updated for 0 words out of 1000\n",
            "Update b..\n",
            "Epoch 24... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 24... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 24... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 25...cost: 2171.118273571139\n",
            "Update W..\n",
            "Epoch 25... W is updated for 0 words out of 1000\n",
            "Update b..\n",
            "Epoch 25... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 25... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 25... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 26...cost: 2170.1572484356348\n",
            "Update W..\n",
            "Epoch 26... W is updated for 0 words out of 1000\n",
            "Update b..\n",
            "Epoch 26... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 26... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 26... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 27...cost: 2169.2673491102246\n",
            "Update W..\n",
            "Epoch 27... W is updated for 0 words out of 1000\n",
            "Update b..\n",
            "Epoch 27... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 27... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 27... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 28...cost: 2168.439169977239\n",
            "Update W..\n",
            "Epoch 28... W is updated for 0 words out of 1000\n",
            "Update b..\n",
            "Epoch 28... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 28... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 28... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 29...cost: 2167.6649653544127\n",
            "Update W..\n",
            "Epoch 29... W is updated for 0 words out of 1000\n",
            "Update b..\n",
            "Epoch 29... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 29... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 29... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 30...cost: 2166.938331448685\n",
            "Update W..\n",
            "Epoch 30... W is updated for 0 words out of 1000\n",
            "Update b..\n",
            "Epoch 30... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 30... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 30... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 31...cost: 2166.2539409860583\n",
            "Update W..\n",
            "Epoch 31... W is updated for 0 words out of 1000\n",
            "Update b..\n",
            "Epoch 31... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 31... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 31... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 32...cost: 2165.607327077789\n",
            "Update W..\n",
            "Epoch 32... W is updated for 0 words out of 1000\n",
            "Update b..\n",
            "Epoch 32... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 32... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 32... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 33...cost: 2164.994711167407\n",
            "Update W..\n",
            "Epoch 33... W is updated for 0 words out of 1000\n",
            "Update b..\n",
            "Epoch 33... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 33... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 33... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 34...cost: 2164.4128686150157\n",
            "Update W..\n",
            "Epoch 34... W is updated for 0 words out of 1000\n",
            "Update b..\n",
            "Epoch 34... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 34... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 34... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 35...cost: 2163.859024936936\n",
            "Update W..\n",
            "Epoch 35... W is updated for 0 words out of 1000\n",
            "Update b..\n",
            "Epoch 35... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 35... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 35... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 36...cost: 2163.330775946932\n",
            "Update W..\n",
            "Epoch 36... W is updated for 0 words out of 1000\n",
            "Update b..\n",
            "Epoch 36... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 36... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 36... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 37...cost: 2162.82602585324\n",
            "Update W..\n",
            "Epoch 37... W is updated for 0 words out of 1000\n",
            "Update b..\n",
            "Epoch 37... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 37... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 37... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 38...cost: 2162.342938486635\n",
            "Update W..\n",
            "Epoch 38... W is updated for 0 words out of 1000\n",
            "Update b..\n",
            "Epoch 38... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 38... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 38... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 39...cost: 2161.8798980190572\n",
            "Update W..\n",
            "Epoch 39... W is updated for 0 words out of 1000\n",
            "Update b..\n",
            "Epoch 39... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 39... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 39... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 40...cost: 2161.4354766012348\n",
            "Update W..\n",
            "Epoch 40... W is updated for 0 words out of 1000\n",
            "Update b..\n",
            "Epoch 40... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 40... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 40... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 41...cost: 2161.0084072034965\n",
            "Update W..\n",
            "Epoch 41... W is updated for 0 words out of 1000\n",
            "Update b..\n",
            "Epoch 41... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 41... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 41... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 42...cost: 2160.597560559113\n",
            "Update W..\n",
            "Epoch 42... W is updated for 0 words out of 1000\n",
            "Update b..\n",
            "Epoch 42... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 42... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 42... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 43...cost: 2160.201925504767\n",
            "Update W..\n",
            "Epoch 43... W is updated for 0 words out of 1000\n",
            "Update b..\n",
            "Epoch 43... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 43... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 43... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 44...cost: 2159.820592235468\n",
            "Update W..\n",
            "Epoch 44... W is updated for 0 words out of 1000\n",
            "Update b..\n",
            "Epoch 44... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 44... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 44... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 45...cost: 2159.452738097755\n",
            "Update W..\n",
            "Epoch 45... W is updated for 0 words out of 1000\n",
            "Update b..\n",
            "Epoch 45... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 45... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 45... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 46...cost: 2159.0976155871926\n",
            "Update W..\n",
            "Epoch 46... W is updated for 0 words out of 1000\n",
            "Update b..\n",
            "Epoch 46... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 46... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 46... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 47...cost: 2158.754542233634\n",
            "Update W..\n",
            "Epoch 47... W is updated for 0 words out of 1000\n",
            "Update b..\n",
            "Epoch 47... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 47... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 47... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 48...cost: 2158.422892075261\n",
            "Update W..\n",
            "Epoch 48... W is updated for 0 words out of 1000\n",
            "Update b..\n",
            "Epoch 48... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 48... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 48... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 49...cost: 2158.102088450765\n",
            "Update W..\n",
            "Epoch 49... W is updated for 0 words out of 1000\n",
            "Update b..\n",
            "Epoch 49... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 49... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 49... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 50...cost: 2157.7915978786377\n",
            "Update W..\n",
            "Epoch 50... W is updated for 0 words out of 1000\n",
            "Update b..\n",
            "Epoch 50... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 50... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 50... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 51...cost: 2157.490924837581\n",
            "Update W..\n",
            "Epoch 51... W is updated for 0 words out of 1000\n",
            "Update b..\n",
            "Epoch 51... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 51... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 51... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 52...cost: 2157.19960730558\n",
            "Update W..\n",
            "Epoch 52... W is updated for 0 words out of 1000\n",
            "Update b..\n",
            "Epoch 52... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 52... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 52... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 53...cost: 2156.9172129510825\n",
            "Update W..\n",
            "Epoch 53... W is updated for 0 words out of 1000\n",
            "Update b..\n",
            "Epoch 53... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 53... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 53... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 54...cost: 2156.643335895312\n",
            "Update W..\n",
            "Epoch 54... W is updated for 0 words out of 1000\n",
            "Update b..\n",
            "Epoch 54... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 54... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 54... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 55...cost: 2156.377593979842\n",
            "Update W..\n",
            "Epoch 55... W is updated for 0 words out of 1000\n",
            "Update b..\n",
            "Epoch 55... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 55... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 55... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 56...cost: 2156.1196264807622\n",
            "Update W..\n",
            "Epoch 56... W is updated for 0 words out of 1000\n",
            "Update b..\n",
            "Epoch 56... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 56... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 56... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 57...cost: 2155.8690922128726\n",
            "Update W..\n",
            "Epoch 57... W is updated for 0 words out of 1000\n",
            "Update b..\n",
            "Epoch 57... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 57... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 57... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 58...cost: 2155.6256679677226\n",
            "Update W..\n",
            "Epoch 58... W is updated for 0 words out of 1000\n",
            "Update b..\n",
            "Epoch 58... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 58... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 58... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 59...cost: 2155.389047229917\n",
            "Update W..\n",
            "Epoch 59... W is updated for 0 words out of 1000\n",
            "Update b..\n",
            "Epoch 59... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 59... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 59... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 60...cost: 2155.1589391183697\n",
            "Update W..\n",
            "Epoch 60... W is updated for 0 words out of 1000\n",
            "Update b..\n",
            "Epoch 60... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 60... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 60... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 61...cost: 2154.935067503269\n",
            "Update W..\n",
            "Epoch 61... W is updated for 0 words out of 1000\n",
            "Update b..\n",
            "Epoch 61... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 61... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 61... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 62...cost: 2154.7171702550804\n",
            "Update W..\n",
            "Epoch 62... W is updated for 0 words out of 1000\n",
            "Update b..\n",
            "Epoch 62... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 62... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 62... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 63...cost: 2154.5049985886035\n",
            "Update W..\n",
            "Epoch 63... W is updated for 0 words out of 1000\n",
            "Update b..\n",
            "Epoch 63... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 63... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 63... b_tilde is updated for 0 words out of 1000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "<font color=green>Q12:</font>\n",
        "<br><font color='green'>\n",
        " Plot the list of losses at the end of each iteration in Algorithm 3.\n",
        "</font>\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "0cmFRYw-D95S"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "L7br03pNbwRG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sX56bVMHkFnd"
      },
      "source": [
        "## 2.3 Learning the weights using gradient descent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Luje3fIdkFsY"
      },
      "source": [
        "In this section, we would like to estimate the parameters of the model using gradient descent.\n",
        "\n",
        "Let $N_{\\text{epochs}}$ be the number of epochs and $\\eta$ be the learning rate.\n",
        "We get the following training algorithm:\n",
        "\n",
        "\n",
        "<center><img width=\"500\" src = \"https://drive.google.com/uc?export=view&id=1Od3xCvMWKOBhMpccmKOtoY3aT3UTJB5Z\"></center>\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "<font color=green>Q13:</font>\n",
        "<br><font color='green'>\n",
        "Implement the gradient descent training algorithm (Algorithm 4).\n",
        "</font>\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "85Ih7k7OD-py"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize weights\n",
        "W = np.random.randn(V, D) / np.sqrt(V + D)\n",
        "b = np.zeros(V)\n",
        "W_tilde = np.random.randn(V,D) / np.sqrt(V + D)\n",
        "b_tilde = np.zeros(V)\n",
        "\n",
        "\n",
        "costs = []\n",
        "for epoch in range(epochs):\n",
        "    # epsilon (V, V) matrix such that epsilon_{ij} = logX_{ij} - W_i^T W_tilde_j - b_i - b_tilde_j\n",
        "    epsilon = logX - W.dot(W_tilde.T) - b.reshape(V, 1) - b_tilde.reshape(1, V)\n",
        "    # cost function sum_{ij} fX_{ij} (logX_{ij} - W_i^T W_tilde_j - b_i - b_tilde_j)^2 = sum_{ij} fX_{ij} epsilon_{ij}^2\n",
        "    cost = (fX * epsilon * epsilon).sum()\n",
        "    costs.append(cost)\n",
        "    print(\"epoch: {}...cost: {}\".format(epoch, cost))\n",
        "\n",
        "    # Update W\n",
        "    print(\"Update W..\")\n",
        "    for i in range(V):\n",
        "        if i%1000==0:\n",
        "            print(\"Epoch {}... W is updated for {} words out of {}\".format(epoch, i, V))\n",
        "        # W_i -= learning_rate*(-2*sum_{j'} fX_{i,j'}*epsilon_{i,j'}*W_tilde_j')\n",
        "        W[i] -= -2*learning_rate*(fX[i,:]*epsilon[i,:]).dot(W_tilde)\n",
        "\n",
        "\n",
        "    # update b\n",
        "    for i in range(V):\n",
        "        if i%1000==0:\n",
        "            print(\"Epoch {}... b is updated for {} words out of {}\".format(epoch, i, V))\n",
        "        # b_i -= learning_rate*(-2*sum_{j'} fX_{i,j'}*epsilon_{i,j'}\n",
        "        b[i] -= -2*learning_rate*fX[i,:].dot(epsilon[i,:])\n",
        "\n",
        "\n",
        "    # update W_tilde\n",
        "    print(\"Update W_tilde..\")\n",
        "    for j in range(V):\n",
        "        if j%1000==0:\n",
        "            print(\"Epoch {}... W_tilde is updated for {} words out of {}\".format(epoch, j, V))\n",
        "        # W_tilde_j -= learning_rate*(-2*sum_{i'} fX_{i',j}*epsilon_{i',j}*W_i')\n",
        "        W_tilde[j] -= -2*learning_rate*(fX[:,j]*epsilon[:,j]).dot(W)\n",
        "\n",
        "\n",
        "    # update b_tilde\n",
        "    print(\"Update b_tilde..\")\n",
        "    for j in range(V):\n",
        "        if j%1000==0:\n",
        "            print(\"Epoch {}... b_tilde is updated for {} words out of {}\".format(epoch, j, V))\n",
        "        # b_tilde_j -= learning_rate*(-2*sum_{i'} fX_{i',j}*epsilon_{i',j})\n",
        "        b_tilde[j] -= -2*learning_rate*fX[:,j].dot(epsilon[:,j])\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "xY4e1IP3mKL4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f567c55-85b5-4742-ddb9-024f01bd64c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 0...cost: 436205.1198939403\n",
            "Update W..\n",
            "Epoch 0... W is updated for 0 words out of 1000\n",
            "Epoch 0... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 0... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 0... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 1...cost: 344880.8003566111\n",
            "Update W..\n",
            "Epoch 1... W is updated for 0 words out of 1000\n",
            "Epoch 1... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 1... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 1... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 2...cost: 278228.90251495404\n",
            "Update W..\n",
            "Epoch 2... W is updated for 0 words out of 1000\n",
            "Epoch 2... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 2... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 2... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 3...cost: 228947.13501177594\n",
            "Update W..\n",
            "Epoch 3... W is updated for 0 words out of 1000\n",
            "Epoch 3... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 3... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 3... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 4...cost: 192013.74308442394\n",
            "Update W..\n",
            "Epoch 4... W is updated for 0 words out of 1000\n",
            "Epoch 4... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 4... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 4... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 5...cost: 163948.0224689369\n",
            "Update W..\n",
            "Epoch 5... W is updated for 0 words out of 1000\n",
            "Epoch 5... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 5... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 5... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 6...cost: 142317.4513039812\n",
            "Update W..\n",
            "Epoch 6... W is updated for 0 words out of 1000\n",
            "Epoch 6... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 6... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 6... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 7...cost: 125407.42176016598\n",
            "Update W..\n",
            "Epoch 7... W is updated for 0 words out of 1000\n",
            "Epoch 7... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 7... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 7... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 8...cost: 111998.64163586106\n",
            "Update W..\n",
            "Epoch 8... W is updated for 0 words out of 1000\n",
            "Epoch 8... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 8... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 8... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 9...cost: 101216.17034358351\n",
            "Update W..\n",
            "Epoch 9... W is updated for 0 words out of 1000\n",
            "Epoch 9... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 9... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 9... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 10...cost: 92426.35980161393\n",
            "Update W..\n",
            "Epoch 10... W is updated for 0 words out of 1000\n",
            "Epoch 10... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 10... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 10... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 11...cost: 85166.00964442869\n",
            "Update W..\n",
            "Epoch 11... W is updated for 0 words out of 1000\n",
            "Epoch 11... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 11... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 11... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 12...cost: 79093.3146381278\n",
            "Update W..\n",
            "Epoch 12... W is updated for 0 words out of 1000\n",
            "Epoch 12... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 12... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 12... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 13...cost: 73953.64741222707\n",
            "Update W..\n",
            "Epoch 13... W is updated for 0 words out of 1000\n",
            "Epoch 13... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 13... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 13... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 14...cost: 69555.50783818476\n",
            "Update W..\n",
            "Epoch 14... W is updated for 0 words out of 1000\n",
            "Epoch 14... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 14... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 14... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 15...cost: 65753.48797024674\n",
            "Update W..\n",
            "Epoch 15... W is updated for 0 words out of 1000\n",
            "Epoch 15... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 15... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 15... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 16...cost: 62436.112746583865\n",
            "Update W..\n",
            "Epoch 16... W is updated for 0 words out of 1000\n",
            "Epoch 16... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 16... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 16... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 17...cost: 59517.0940246058\n",
            "Update W..\n",
            "Epoch 17... W is updated for 0 words out of 1000\n",
            "Epoch 17... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 17... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 17... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 18...cost: 56928.99176431354\n",
            "Update W..\n",
            "Epoch 18... W is updated for 0 words out of 1000\n",
            "Epoch 18... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 18... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 18... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 19...cost: 54618.58528871401\n",
            "Update W..\n",
            "Epoch 19... W is updated for 0 words out of 1000\n",
            "Epoch 19... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 19... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 19... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 20...cost: 52543.468292222344\n",
            "Update W..\n",
            "Epoch 20... W is updated for 0 words out of 1000\n",
            "Epoch 20... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 20... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 20... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 21...cost: 50669.525879658584\n",
            "Update W..\n",
            "Epoch 21... W is updated for 0 words out of 1000\n",
            "Epoch 21... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 21... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 21... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 22...cost: 48969.051819380016\n",
            "Update W..\n",
            "Epoch 22... W is updated for 0 words out of 1000\n",
            "Epoch 22... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 22... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 22... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 23...cost: 47419.33368238033\n",
            "Update W..\n",
            "Epoch 23... W is updated for 0 words out of 1000\n",
            "Epoch 23... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 23... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 23... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 24...cost: 46001.582209074106\n",
            "Update W..\n",
            "Epoch 24... W is updated for 0 words out of 1000\n",
            "Epoch 24... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 24... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 24... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 25...cost: 44700.11557084185\n",
            "Update W..\n",
            "Epoch 25... W is updated for 0 words out of 1000\n",
            "Epoch 25... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 25... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 25... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 26...cost: 43501.73356844667\n",
            "Update W..\n",
            "Epoch 26... W is updated for 0 words out of 1000\n",
            "Epoch 26... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 26... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 26... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 27...cost: 42395.23423528272\n",
            "Update W..\n",
            "Epoch 27... W is updated for 0 words out of 1000\n",
            "Epoch 27... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 27... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 27... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 28...cost: 41371.03785296246\n",
            "Update W..\n",
            "Epoch 28... W is updated for 0 words out of 1000\n",
            "Epoch 28... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 28... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 28... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 29...cost: 40420.89246731785\n",
            "Update W..\n",
            "Epoch 29... W is updated for 0 words out of 1000\n",
            "Epoch 29... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 29... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 29... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 30...cost: 39537.641608660626\n",
            "Update W..\n",
            "Epoch 30... W is updated for 0 words out of 1000\n",
            "Epoch 30... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 30... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 30... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 31...cost: 38715.03976825406\n",
            "Update W..\n",
            "Epoch 31... W is updated for 0 words out of 1000\n",
            "Epoch 31... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 31... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 31... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 32...cost: 37947.60475546787\n",
            "Update W..\n",
            "Epoch 32... W is updated for 0 words out of 1000\n",
            "Epoch 32... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 32... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 32... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 33...cost: 37230.49870666132\n",
            "Update W..\n",
            "Epoch 33... W is updated for 0 words out of 1000\n",
            "Epoch 33... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 33... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 33... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 34...cost: 36559.43148743077\n",
            "Update W..\n",
            "Epoch 34... W is updated for 0 words out of 1000\n",
            "Epoch 34... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 34... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 34... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 35...cost: 35930.58170431702\n",
            "Update W..\n",
            "Epoch 35... W is updated for 0 words out of 1000\n",
            "Epoch 35... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 35... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 35... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 36...cost: 35340.531650513694\n",
            "Update W..\n",
            "Epoch 36... W is updated for 0 words out of 1000\n",
            "Epoch 36... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 36... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 36... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 37...cost: 34786.21334724404\n",
            "Update W..\n",
            "Epoch 37... W is updated for 0 words out of 1000\n",
            "Epoch 37... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 37... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 37... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 38...cost: 34264.86347753991\n",
            "Update W..\n",
            "Epoch 38... W is updated for 0 words out of 1000\n",
            "Epoch 38... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 38... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 38... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 39...cost: 33773.985493082786\n",
            "Update W..\n",
            "Epoch 39... W is updated for 0 words out of 1000\n",
            "Epoch 39... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 39... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 39... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 40...cost: 33311.31754515841\n",
            "Update W..\n",
            "Epoch 40... W is updated for 0 words out of 1000\n",
            "Epoch 40... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 40... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 40... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 41...cost: 32874.80517553604\n",
            "Update W..\n",
            "Epoch 41... W is updated for 0 words out of 1000\n",
            "Epoch 41... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 41... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 41... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 42...cost: 32462.577922993765\n",
            "Update W..\n",
            "Epoch 42... W is updated for 0 words out of 1000\n",
            "Epoch 42... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 42... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 42... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 43...cost: 32072.929171821357\n",
            "Update W..\n",
            "Epoch 43... W is updated for 0 words out of 1000\n",
            "Epoch 43... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 43... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 43... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 44...cost: 31704.298701615447\n",
            "Update W..\n",
            "Epoch 44... W is updated for 0 words out of 1000\n",
            "Epoch 44... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 44... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 44... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 45...cost: 31355.257501832228\n",
            "Update W..\n",
            "Epoch 45... W is updated for 0 words out of 1000\n",
            "Epoch 45... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 45... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 45... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 46...cost: 31024.494496537107\n",
            "Update W..\n",
            "Epoch 46... W is updated for 0 words out of 1000\n",
            "Epoch 46... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 46... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 46... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 47...cost: 30710.804889636038\n",
            "Update W..\n",
            "Epoch 47... W is updated for 0 words out of 1000\n",
            "Epoch 47... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 47... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 47... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 48...cost: 30413.07989243435\n",
            "Update W..\n",
            "Epoch 48... W is updated for 0 words out of 1000\n",
            "Epoch 48... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 48... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 48... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 49...cost: 30130.29763658756\n",
            "Update W..\n",
            "Epoch 49... W is updated for 0 words out of 1000\n",
            "Epoch 49... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 49... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 49... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 50...cost: 29861.51510863451\n",
            "Update W..\n",
            "Epoch 50... W is updated for 0 words out of 1000\n",
            "Epoch 50... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 50... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 50... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 51...cost: 29605.860969074314\n",
            "Update W..\n",
            "Epoch 51... W is updated for 0 words out of 1000\n",
            "Epoch 51... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 51... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 51... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 52...cost: 29362.52914070354\n",
            "Update W..\n",
            "Epoch 52... W is updated for 0 words out of 1000\n",
            "Epoch 52... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 52... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 52... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 53...cost: 29130.77306870514\n",
            "Update W..\n",
            "Epoch 53... W is updated for 0 words out of 1000\n",
            "Epoch 53... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 53... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 53... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 54...cost: 28909.90056958935\n",
            "Update W..\n",
            "Epoch 54... W is updated for 0 words out of 1000\n",
            "Epoch 54... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 54... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 54... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 55...cost: 28699.269198154438\n",
            "Update W..\n",
            "Epoch 55... W is updated for 0 words out of 1000\n",
            "Epoch 55... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 55... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 55... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 56...cost: 28498.28207166282\n",
            "Update W..\n",
            "Epoch 56... W is updated for 0 words out of 1000\n",
            "Epoch 56... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 56... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 56... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 57...cost: 28306.38409880086\n",
            "Update W..\n",
            "Epoch 57... W is updated for 0 words out of 1000\n",
            "Epoch 57... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 57... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 57... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 58...cost: 28123.0585680232\n",
            "Update W..\n",
            "Epoch 58... W is updated for 0 words out of 1000\n",
            "Epoch 58... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 58... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 58... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 59...cost: 27947.824055812856\n",
            "Update W..\n",
            "Epoch 59... W is updated for 0 words out of 1000\n",
            "Epoch 59... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 59... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 59... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 60...cost: 27780.231620420214\n",
            "Update W..\n",
            "Epoch 60... W is updated for 0 words out of 1000\n",
            "Epoch 60... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 60... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 60... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 61...cost: 27619.862250928196\n",
            "Update W..\n",
            "Epoch 61... W is updated for 0 words out of 1000\n",
            "Epoch 61... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 61... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 61... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 62...cost: 27466.324545158554\n",
            "Update W..\n",
            "Epoch 62... W is updated for 0 words out of 1000\n",
            "Epoch 62... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 62... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 62... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 63...cost: 27319.25259308362\n",
            "Update W..\n",
            "Epoch 63... W is updated for 0 words out of 1000\n",
            "Epoch 63... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 63... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 63... b_tilde is updated for 0 words out of 1000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "<font color=green>Q14:</font>\n",
        "<br><font color='green'>\n",
        " Plot the list of losses at the end of each iteration in Algorithm 4.\n",
        "</font>\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "6aS7uO42D_Ya"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5VRIWLWrmfnR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 641
        },
        "outputId": "4fe5fd0c-4d1e-46e4-f61f-5e2b64f8ed02"
      },
      "source": [
        "# Plot the costs for each epoch\n",
        "fig = plt.figure(figsize=(10, 7))\n",
        "plt.plot(costs)\n",
        "plt.xlabel(\"epochs\")\n",
        "plt.ylabel(\"cost\")\n",
        "plt.title(\"Cost function using gradient descent\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x700 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sd9PcTFwkFwo"
      },
      "source": [
        "# 3. Exercise: Introducing regularization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zWkYPM1dkF1u"
      },
      "source": [
        "Let us introduce a regularization penalty term in the cost function. The new cost function is defined as follows:\n",
        "\n",
        "\n",
        "\\begin{equation*}\n",
        "\\tilde{J} = \\sum\\limits_{i=1}^V \\sum\\limits_{j=1}^V f(X_{ij}) (\\log X_{ij} - W_i^T \\tilde{W}_j - b_i - \\tilde{b}_j)^2 + \\lambda \\left( ||W||_{\\text{F}}^2 +   ||\\tilde{W}||_{\\text{F}}^2 + ||b||_2^2 + ||\\tilde{b} ||_2^2 \\right)\n",
        "\\end{equation*}\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "<font color=green>Q15:</font>\n",
        "<br><font color='green'>\n",
        "Show that:\n",
        "\\begin{equation}\n",
        "||W||_{\\text{F}}^2 = \\sum\\limits_{i=1}^V W_i^T W_i\n",
        "\\end{equation}\n",
        "\n",
        "</font>\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "0Fgcb-ASEAad"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QT6sJx8wTDaE"
      },
      "source": [
        "---\n",
        "**Solution:**\n",
        "\n",
        "Let $W = [w_{ij}]_{ij}$.\n",
        "\n",
        "For all $v \\in \\{1, \\dots, V \\} \\quad W_v = [w_{v1}, \\dots, w_{vD}]^T \\in \\mathcal{M}_{D, 1}(\\mathbb{R})$.\n",
        "\n",
        "Therefore:\n",
        "\n",
        "\\begin{align}\n",
        "||W||_{\\text{F}}^2 &= \\sum\\limits_{v=1}^{V} \\left( \\sum\\limits_{d=1}^{D} |w_{vd}|^2 \\right) \\\\\n",
        "&=  \\sum\\limits_{v=1}^{V} W_v^T W_v\n",
        "\\end{align}\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "<font color=green>Q16:</font>\n",
        "<br><font color='green'>\n",
        "Deduce that for all $i \\in \\{1, \\dots, V \\}$:\n",
        "\\begin{align}\n",
        "& \\nabla_{W_i} (||W||_{\\text{F}}^2) = 2W_i \\quad \\text{(3.1)} \\\\\n",
        "&(\\text{Hint}: \\forall z \\in \\mathbb{R}^D \\ \\forall A \\in \\mathcal{M}_{D, D}(\\mathbb{R}) \\quad \\nabla_z (z^T A z) = (A + A^T)z )\n",
        "\\end{align}\n",
        "\n",
        "</font>\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "JgdEJEcFEBCh"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Wcp4if-VP9y"
      },
      "source": [
        "---\n",
        "**Solution:**\n",
        "\n",
        "For all $i \\in \\{1, \\dots, V \\}$:\n",
        "\n",
        "\n",
        "\\begin{align}\n",
        " \\nabla_{W_i} (||W||_{\\text{F}}^2) &= \\nabla_{W_i} \\left( \\sum\\limits_{v=1}^{V} W_v^T W_v \\right) \\\\\n",
        " &=  \\sum\\limits_{v=1}^{V} \\nabla_{W_i} \\left( W_v^T W_v \\right) \\\\\n",
        " &=  \\nabla_{W_i} \\left( W_i^T I_D W_i \\right) \\\\\n",
        " &=  (I_D + I_D^T)W_i \\\\\n",
        " &= 2 W_i\n",
        "\\end{align}\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "<font color=green>Q17:</font>\n",
        "<br><font color='green'>\n",
        "From the equations(2.1), (2.2), (2.3), (2.4) and (3.1), show that the update equations for the method of alternating least squares become:\n",
        "\n",
        "\\begin{align*}\n",
        "&W_i^{(t+1)} \\longleftarrow \\left( \\sum_{j'=1}^V f(X_{ij'}) \\tilde{W}_{j'}^{(t)} \\tilde{W}_{j'}^{(t)^T} + \\lambda I_D \\right)^{-1} \\left( \\sum_{j'=1}^{V} f(X_{ij'})( \\log X_{ij'} - b_i^{(t)} - \\tilde{b}_{j'}^{(t)}) \\tilde{W}_{j'}^{(t)} \\right)  \\\\\n",
        "&\\tilde{W}_j^{(t+1)} \\longleftarrow \\left( \\sum_{i'=1}^V f(X_{i' j}) W_{i'}^{(t)} W_{i'}^{(t)^T} + \\lambda I_D \\right)^{-1} \\left( \\sum_{i'=1}^{V} f(X_{i' j})( \\log X_{i' j} - b_{i'}^{(t)} - \\tilde{b}_{j}^{(t)}) W_{i'}^{(t)} \\right) \\\\\n",
        "&b_i^{(t+1)} \\longleftarrow \\left( \\sum_{j'=1}^V f(X_{ij'}) + \\lambda  \\right)^{-1} \\left( \\sum_{j'=1}^{V} f(X_{ij'})( \\log X_{ij'} - W_i^{(t)^T} \\tilde{W}_{j'}^{(t)} - \\tilde{b}_{j'}^{(t)}) \\right)  \\\\\n",
        "&\\tilde{b}_j^{(t+1)} \\longleftarrow \\left( \\sum_{i'=1}^V f(X_{i' j}) + \\lambda  \\right)^{-1} \\left( \\sum_{i'=1}^{V} f(X_{i' j})( \\log X_{i' j} - W_{i'}^{(t)^T} \\tilde{W}_{j}^{(t)} - b_{i'}^{(t)}) \\right)\n",
        "\\end{align*}\n",
        "</font>\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "HcfbBXP2EBpp"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lTCLv_fKcEej"
      },
      "source": [
        "---\n",
        "**Solution:**\n",
        "\n",
        "\n",
        "Let us compute the gradients of the cost function $\\tilde{J}$ with respect to all the parameters:\n",
        "\n",
        "\n",
        "For all $i \\in \\{1, \\dots, V \\}$ and all $j \\in \\{ 1, \\dots, V \\}$:\n",
        "\n",
        "\n",
        "\\begin{align}\n",
        "& \\nabla_{W_i} \\tilde{J}(W_i) = -2 \\sum_{j'=1}^V f(X_{ij'}) \\left( \\log X_{ij'} - W_i^T \\tilde{W}_{j'} - b_i - \\tilde{b}_{j'} \\right) \\tilde{W}_{j'} + 2 \\lambda W_i \\quad \\text{(From (2.1) and Question 18)} \\\\\n",
        "& \\nabla_{\\tilde{W}_j} \\tilde{J}(W_j) = -2 \\sum_{i'=1}^V f(X_{i' j}) \\left( \\log X_{i' j} - W_{i'}^T \\tilde{W}_j - b_{i'} - \\tilde{b}_j \\right) W_{i'} + 2 \\lambda \\tilde{W}_j \\quad \\text{(From (2.2) and Question 18)}  \\\\\n",
        "&\\nabla_{b_i} \\tilde{J}(b_i) = -2 \\sum_{j'=1}^V f(X_{ij'}) \\left( \\log X_{ij'} - W_i^T \\tilde{W}_{j'} - b_i - \\tilde{b}_{j'} \\right) + 2 \\lambda b_i \\quad \\text{(From (2.3)} \\\\\n",
        "& \\nabla_{\\tilde{b}_j} \\tilde{J}(\\tilde{b}_j) = -2 \\sum_{i'=1}^V f(X_{i' j}) \\left( \\log X_{i' j} - W_{i'}^T \\tilde{W}_j - b_{i'} - \\tilde{b}_j \\right) + 2 \\lambda \\tilde{b}_j \\quad \\text{(From (2.4)}\n",
        "\\end{align}\n",
        "\n",
        "Let us deduce the update equations:\n",
        "\n",
        "* For $W_i$:\n",
        "\\begin{align*}\n",
        "\\nabla_{W_i} J(W_i) = 0 & \\iff -2 \\sum_{j'=1}^V f(X_{ij'}) \\left( \\log X_{ij'} - W_i^T \\tilde{W}_{j'} - b_i - \\tilde{b}_{j'} \\right) \\tilde{W}_{j'} + 2 \\lambda W_i = 0  \\\\\n",
        "& \\iff \\sum_{j'=1}^V f(X_{ij'}) \\left( \\log X_{ij'} - b_i - \\tilde{b}_{j'} \\right) \\tilde{W}_{j'} = \\sum_{j'=1}^V f(X_{ij'})  W_i^T \\tilde{W}_{j'} \\tilde{W}_{j'} + \\lambda W_i \\\\\n",
        "& \\iff \\sum_{j'=1}^V f(X_{ij'}) \\left( \\log X_{ij'} - b_i - \\tilde{b}_{j'} \\right) \\tilde{W}_{j'} = \\left( \\sum_{j'=1}^V f(X_{ij'})  \\tilde{W}_{j'}^T \\tilde{W}_{j'} + \\lambda I_D \\right) W_i   \\\\\n",
        "& \\iff W_i = \\left( \\sum_{j'=1}^V f(X_{ij'}) \\tilde{W}_{j'} \\tilde{W}_{j'}^T + \\lambda I_D \\right)^{-1} \\left( \\sum_{j'=1}^{V} f(X_{ij'})( \\log X_{ij'} - b_i - \\tilde{b}_{j'}) \\tilde{W}_{j'} \\right)\n",
        "\\end{align*}\n",
        "\n",
        "* For $\\tilde{W}_{j}$:\n",
        "\n",
        "\\begin{align*}\n",
        "\\nabla_{\\tilde{W}_j} J(\\tilde{W}_j) = 0 & \\iff  -2 \\sum_{i'=1}^V f(X_{i' j}) \\left( \\log X_{i' j} - W_{i'}^T \\tilde{W}_j - b_{i'} - \\tilde{b}_j \\right) + 2 \\lambda \\tilde{W}_j = 0   \\\\\n",
        "&\\iff \\sum_{i'=1}^V f(X_{i' j}) \\left( \\log X_{i' j} - b_{i'} - \\tilde{b}_j \\right) W_{i'} = \\sum_{i'=1}^V f(X_{i' j})  W_{i'}^T \\tilde{W}_j  W_{i'} + \\lambda \\tilde{W}_j \\\\\n",
        "& \\iff \\sum_{i'=1}^V f(X_{i' j}) \\left( \\log X_{i' j} - b_{i'} - \\tilde{b}_j \\right) W_{i'} = \\left( \\sum_{i'=1}^V f(X_{i' j})  W_{i'} W_{i'}^T + \\lambda I_D \\right)  \\tilde{W}_j    \\\\\n",
        "& \\iff\\tilde{W}_j = \\left( \\sum_{i'=1}^V f(X_{i' j}) W_{i'} W_{i'}^T + \\lambda I_D \\right)^{-1} \\left( \\sum_{i'=1}^{V} f(X_{i' j})( \\log X_{i' j} - b_{i'} - \\tilde{b}_{j}) W_{i'} \\right)\n",
        "\\end{align*}\n",
        "\n",
        "* For $b_i$:\n",
        "\\begin{align*}\n",
        "\\nabla_{b_i} J(b_i) = 0  & \\iff -2 \\sum_{j'=1}^V f(X_{ij'}) \\left( \\log X_{ij'} - W_i^T \\tilde{W}_{j'} - b_i - \\tilde{b}_{j'} \\right) + 2 \\lambda b_i = 0 \\\\\n",
        "& \\iff \\sum_{j'=1}^V f(X_{ij'}) \\left( \\log X_{ij'} - W_i^T \\tilde{W}_{j'} - \\tilde{b}_{j'} \\right)  = \\left( \\sum_{j'=1}^V f(X_{ij'}) + \\lambda \\right) b_i \\\\\n",
        "& \\iff b_i = \\left( \\sum_{j'=1}^V f(X_{ij'}) + \\lambda \\right)^{-1} \\left( \\sum_{j'=1}^{V} f(X_{ij'})( \\log X_{ij'} - W_i^T \\tilde{W}_{j'} - \\tilde{b}_{j'}) \\right)\n",
        "\\end{align*}\n",
        "\n",
        "* For $\\tilde{b}_j$:\n",
        "\\begin{align*}\n",
        "\\nabla_{\\tilde{b}_j} J(\\tilde{b}_j) = 0  & \\iff -2 \\sum_{i'=1}^V f(X_{i' j}) \\left( \\log X_{i' j} - W_{i'}^T \\tilde{W}_j - b_{i'} - \\tilde{b}_j \\right) + 2 \\lambda \\tilde{b}_j  = 0  \\\\\n",
        "& \\iff \\sum_{i'=1}^V f(X_{i' j}) \\left( \\log X_{i' j} - W_{i'}^T \\tilde{W}_j - b_{i'} -  \\right) = \\left(\\sum_{i'=1}^V f(X_{i' j}) + \\lambda \\right) \\tilde{b}_j \\\\\n",
        "& \\iff \\tilde{b}_j = \\left( \\sum_{i'=1}^V f(X_{i' j}) + \\lambda \\right)^{-1} \\left( \\sum_{i'=1}^{V} f(X_{i' j})( \\log X_{i' j} - W_{i'}^T \\tilde{W}_{j} - b_{i'}) \\right)\n",
        "\\end{align*}\n",
        "\n",
        "We deduce the following update equations:\n",
        "\n",
        "\\begin{align*}\n",
        "&W_i^{(t+1)} \\longleftarrow \\left( \\sum_{j'=1}^V f(X_{ij'}) \\tilde{W}_{j'}^{(t)} \\tilde{W}_{j'}^{(t)^T} + \\lambda I_D \\right)^{-1} \\left( \\sum_{j'=1}^{V} f(X_{ij'})( \\log X_{ij'} - b_i^{(t)} - \\tilde{b}_{j'}^{(t)}) \\tilde{W}_{j'}^{(t)} \\right)  \\\\\n",
        "&\\tilde{W}_j^{(t+1)} \\longleftarrow \\left( \\sum_{i'=1}^V f(X_{i' j}) W_{i'}^{(t)} W_{i'}^{(t)^T} + \\lambda I_D \\right)^{-1} \\left( \\sum_{i'=1}^{V} f(X_{i' j})( \\log X_{i' j} - b_{i'}^{(t)} - \\tilde{b}_{j}^{(t)}) W_{i'}^{(t)} \\right) \\\\\n",
        "&b_i^{(t+1)} \\longleftarrow \\left( \\sum_{j'=1}^V f(X_{ij'}) + \\lambda  \\right)^{-1} \\left( \\sum_{j'=1}^{V} f(X_{ij'})( \\log X_{ij'} - W_i^{(t)^T} \\tilde{W}_{j'}^{(t)} - \\tilde{b}_{j'}^{(t)}) \\right)  \\\\\n",
        "&\\tilde{b}_j^{(t+1)} \\longleftarrow \\left( \\sum_{i'=1}^V f(X_{i' j}) + \\lambda  \\right)^{-1} \\left( \\sum_{i'=1}^{V} f(X_{i' j})( \\log X_{i' j} - W_{i'}^{(t)^T} \\tilde{W}_{j}^{(t)} - b_{i'}^{(t)}) \\right)\n",
        "\\end{align*}\n",
        "\n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "<font color=green>Q18:</font>\n",
        "<br><font color='green'>\n",
        " What would be the update equations for minimizing the new loss function $\\tilde{J}$ by using the gradient descent algorithm.\n",
        "</font>\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "OcsjfGD9EC2C"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-7Z2AL-hU2w"
      },
      "source": [
        "---\n",
        "**Solution:**\n",
        "\n",
        "We will set a number of epochs $N_{\\text{epochs}}$ and a learning rate $\\eta$.\n",
        "\n",
        "* Initialize randomly $W^{(0)}, \\tilde{W}^{(0)}, b^{(0)}, \\tilde{b}^{(0)}$\n",
        "* For $t \\in \\{0, \\dots, N_{\\text{epochs}}-2\\}$:\n",
        "  * For $i \\in \\{0, \\dots, V-1 \\}$:\n",
        "  \\begin{align}\n",
        "  &W_i^{(t+1)} \\longleftarrow W_i^{(t)} - \\eta \\nabla_{W_i} \\tilde{J} (W_i^{(t)}) \\\\\n",
        "  &\\text{with} \\quad \\nabla_{W_i} \\tilde{J} (W_i) = -2 \\sum_{j'=1}^V f(X_{ij'}) \\left( \\log X_{ij'} - W_i^T \\tilde{W}_{j'} - b_i - \\tilde{b}_{j'} \\right) \\tilde{W}_{j'} + 2 \\lambda W_i\n",
        "  \\end{align}\n",
        "\n",
        "  * For $j \\in \\{0, \\dots, V-1 \\}$:\n",
        "  \\begin{align}\n",
        "  &\\tilde{W}_j^{(t+1)} \\longleftarrow \\tilde{W}_j^{(t)} - \\eta \\nabla_{\\tilde{W}_j} \\tilde{J} (\\tilde{W}_j^{(t)}) \\\\\n",
        "  &\\text{with} \\quad \\nabla_{\\tilde{W}_j} \\tilde{J} (\\tilde{W}_j) = -2 \\sum_{i'=1}^V f(X_{i' j}) \\left( \\log X_{i' j} - W_{i'}^T \\tilde{W}_j - b_{i'} - \\tilde{b}_j \\right) + 2 \\lambda \\tilde{W}_j\n",
        "  \\end{align}\n",
        "\n",
        "  * For $i \\in \\{0, \\dots, V-1 \\}$:\n",
        "  \\begin{align}\n",
        "  &b_i^{(t+1)} \\longleftarrow b_i^{(t)} - \\eta \\nabla_{b_i} \\tilde{J} (b_i^{(t)}) \\\\\n",
        "  &\\text{with} \\quad \\nabla_{b_i} \\tilde{J} (b_i) = -2 \\sum_{j'=1}^V f(X_{ij'}) \\left( \\log X_{ij'} - W_i^T \\tilde{W}_{j'} - b_i - \\tilde{b}_{j'} \\right) + 2 \\lambda b_i\n",
        "  \\end{align}\n",
        "\n",
        "  * For $j \\in \\{0, \\dots, V-1 \\}$:\n",
        "  \\begin{align}\n",
        "  &\\tilde{b}_j^{(t+1)} \\longleftarrow \\tilde{b}_j^{(t)} - \\eta \\nabla_{\\tilde{b}_j} \\tilde{J} (\\tilde{b}_j^{(t)}) \\\\\n",
        "  &\\text{with} \\quad \\nabla_{\\tilde{b}_j} \\tilde{J} (\\tilde{b}_j) = -2 \\sum_{i'=1}^V f(X_{i' j}) \\left( \\log X_{i' j} - W_{i'}^T \\tilde{W}_j - b_{i'} - \\tilde{b}_j \\right) + 2 \\lambda \\tilde{b}_j\n",
        "  \\end{align}\n",
        "\n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Appendix"
      ],
      "metadata": {
        "id": "I8J7ErjCVRJf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "<br><font color='green'>\n",
        "Let us show that:\n",
        "\n",
        "\\begin{align*}\n",
        "&\\nabla_{W_i} J(W_i) = 0 \\iff W_i = \\left( \\sum_{j'=1}^V f(X_{ij'}) \\tilde{W}_{j'} \\tilde{W}_{j'}^T \\right)^{-1} \\left( \\sum_{j'=1}^{V} f(X_{ij'})( \\log X_{ij'} - b_i - \\tilde{b}_{j'}) \\tilde{W}_{j'} \\right)  \\\\\n",
        "&\\nabla_{\\tilde{W}_j} J(\\tilde{W}_j) = 0 \\iff \\tilde{W}_j = \\left( \\sum_{i'=1}^V f(X_{i' j}) W_{i'} W_{i'}^T \\right)^{-1} \\left( \\sum_{i'=1}^{V} f(X_{i' j})( \\log X_{i' j} - b_{i'} - \\tilde{b}_{j}) W_{i'} \\right)  \\\\\n",
        "&\\nabla_{b_i} J(b_i) = 0 \\iff b_i = \\left( \\sum_{j'=1}^V f(X_{ij'})  \\right)^{-1} \\left( \\sum_{j'=1}^{V} f(X_{ij'})( \\log X_{ij'} - W_i^T \\tilde{W}_{j'} - \\tilde{b}_{j'}) \\right)  \\\\\n",
        "&\\nabla_{\\tilde{b}_j} J(\\tilde{b}_j) = 0 \\iff \\tilde{b}_j = \\left( \\sum_{i'=1}^V f(X_{i' j})  \\right)^{-1} \\left( \\sum_{i'=1}^{V} f(X_{i' j})( \\log X_{i' j} - W_{i'}^T \\tilde{W}_{j} - b_{i'}) \\right)\n",
        "\\end{align*}\n",
        "</font>\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "ZxPxjvefVel3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "**Proof:**\n",
        "\n",
        "First, we need to prove a preliminary result:\n",
        "\n",
        "\\begin{equation*}\n",
        "  \\forall a,b \\in \\mathcal{M}_{D, 1}(\\mathbb{R}) \\quad (a^T b) \\ b = (b \\ b^T) \\ a \\quad (\\Delta)\n",
        "\\end{equation*}\n",
        "\n",
        "\n",
        "Indeed,\n",
        "\n",
        "\\begin{align}\n",
        "\\forall a,b \\in \\mathcal{M}_{D, 1}(\\mathbb{R}) \\quad (a^T b) \\ b  &= b \\ (a^T b) \\\\\n",
        "&= b \\ (b^T a) \\quad (\\text{As} \\ a^Tb \\ \\text{is a scalar, it's equal to its transpose}) \\\\\n",
        "&=  (b b^T) \\ a\n",
        "\\end{align}\n",
        "\n",
        "For all $i \\in \\{1, \\dots, V \\}$ and for all $j \\in \\{1, \\dots, V \\}$.\n",
        "\n",
        "Let us find the optimal parameters by setting the gradients to 0:\n",
        "\n",
        "* For $W_i$:\n",
        "\\begin{align*}\n",
        "\\nabla_{W_i} J(W_i) = 0 & \\iff -2 \\sum_{j'=1}^V f(X_{ij'}) \\left( \\log X_{ij'} - W_i^T \\tilde{W}_{j'} - b_i - \\tilde{b}_{j'} \\right) \\tilde{W}_{j'} = 0 \\quad \\text{(From (2.1))} \\\\\n",
        "& \\iff \\sum_{j'=1}^V f(X_{ij'}) \\left( \\log X_{ij'} - b_i - \\tilde{b}_{j'} \\right) \\tilde{W}_{j'} = \\sum_{j'=1}^V f(X_{ij'})  W_i^T \\tilde{W}_{j'} \\tilde{W}_{j'} \\\\\n",
        "& \\iff \\sum_{j'=1}^V f(X_{ij'}) \\left( \\log X_{ij'} - b_i - \\tilde{b}_{j'} \\right) \\tilde{W}_{j'} = \\left( \\sum_{j'=1}^V f(X_{ij'})  \\tilde{W}_{j'} \\tilde{W}_{j'}^T \\right) W_i  \\quad (\\text{From} \\ (\\Delta)) \\\\\n",
        "& \\iff W_i = \\left( \\sum_{j'=1}^V f(X_{ij'}) \\tilde{W}_{j'} \\tilde{W}_{j'}^T \\right)^{-1} \\left( \\sum_{j'=1}^{V} f(X_{ij'})( \\log X_{ij'} - b_i - \\tilde{b}_{j'}) \\tilde{W}_{j'} \\right)\n",
        "\\end{align*}\n",
        "\n",
        "* For $\\tilde{W}_{j}$:\n",
        "\n",
        "\\begin{align*}\n",
        "\\nabla_{\\tilde{W}_j} J(\\tilde{W}_j) = 0 & \\iff  -2 \\sum_{i'=1}^V f(X_{i' j}) \\left( \\log X_{i' j} - W_{i'}^T \\tilde{W}_j - b_{i'} - \\tilde{b}_j \\right) = 0  \\quad \\text{(From (2.2))} \\\\\n",
        "&\\iff \\sum_{i'=1}^V f(X_{i' j}) \\left( \\log X_{i' j} - b_{i'} - \\tilde{b}_j \\right) W_{i'} = \\sum_{i'=1}^V f(X_{i' j})  W_{i'}^T \\tilde{W}_j  W_{i'} \\\\\n",
        "& \\iff \\sum_{i'=1}^V f(X_{i' j}) \\left( \\log X_{i' j} - b_{i'} - \\tilde{b}_j \\right) W_{i'} = \\left( \\sum_{i'=1}^V f(X_{i' j})  W_{i'} W_{i'}^T \\right)  \\tilde{W}_j    \\\\\n",
        "& \\iff\\tilde{W}_j = \\left( \\sum_{i'=1}^V f(X_{i' j}) W_{i'} W_{i'}^T \\right)^{-1} \\left( \\sum_{i'=1}^{V} f(X_{i' j})( \\log X_{i' j} - b_{i'} - \\tilde{b}_{j}) W_{i'} \\right)\n",
        "\\end{align*}\n",
        "\n",
        "* For $b_i$:\n",
        "\\begin{align*}\n",
        "\\nabla_{b_i} J(b_i) = 0  & \\iff -2 \\sum_{j'=1}^V f(X_{ij'}) \\left( \\log X_{ij'} - W_i^T \\tilde{W}_{j'} - b_i - \\tilde{b}_{j'} \\right) = 0 \\quad \\text{(From (2.3))} \\\\\n",
        "& \\iff \\sum_{j'=1}^V f(X_{ij'}) \\left( \\log X_{ij'} - W_i^T \\tilde{W}_{j'} - \\tilde{b}_{j'} \\right)  = \\left( \\sum_{j'=1}^V f(X_{ij'}) \\right) b_i \\\\\n",
        "& \\iff b_i = \\left( \\sum_{j'=1}^V f(X_{ij'})  \\right)^{-1} \\left( \\sum_{j'=1}^{V} f(X_{ij'})( \\log X_{ij'} - W_i^T \\tilde{W}_{j'} - \\tilde{b}_{j'}) \\right)\n",
        "\\end{align*}\n",
        "\n",
        "* For $\\tilde{b}_j$:\n",
        "\\begin{align*}\n",
        "\\nabla_{\\tilde{b}_j} J(\\tilde{b}_j) = 0  & \\iff -2 \\sum_{i'=1}^V f(X_{i' j}) \\left( \\log X_{i' j} - W_{i'}^T \\tilde{W}_j - b_{i'} - \\tilde{b}_j \\right) = 0 \\quad \\text{(From (2.4))} \\\\\n",
        "& \\iff \\sum_{i'=1}^V f(X_{i' j}) \\left( \\log X_{i' j} - W_{i'}^T \\tilde{W}_j - b_{i'}   \\right) = \\left(\\sum_{i'=1}^V f(X_{i' j}) \\right) \\tilde{b}_j \\\\\n",
        "& \\iff \\tilde{b}_j = \\left( \\sum_{i'=1}^V f(X_{i' j})  \\right)^{-1} \\left( \\sum_{i'=1}^{V} f(X_{i' j})( \\log X_{i' j} - W_{i'}^T \\tilde{W}_{j} - b_{i'}) \\right)\n",
        "\\end{align*}\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "Fs8YgSFKXIHP"
      }
    }
  ]
}